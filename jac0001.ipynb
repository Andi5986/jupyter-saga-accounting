{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #3933FF; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Importing & Cleansing Data\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "> ### The Excel file contains a journal entry and is generated by SAGA Accounting. \n",
    "> ### Perform some data cleansing to make it user friendly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/andi/Desktop/CPD/REGISTRU JURNAL 29112023 1651.xls'\n",
    "df = pd.read_excel(file_path)\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path, header=8)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['crt.'], inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Explicatie'].str.contains('google', case=False) & (df['debitor'] == 628)]\n",
    "sum_debitor = filtered_df['debitor'].sum()\n",
    "filtered_df, sum_debitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[\n",
    "    df['Explicatie'].str.contains('Google', case=False) \n",
    "    & (df['creditor'] != 512.1)\n",
    "    ]\n",
    "sum_debit = filtered_df['Debit'].sum()\n",
    "sum_debit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #3933FF; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "More Power\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "> ### We connect extrenal libraries\n",
    "> ### We bring the currency to the journal entry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_historical_rates(currency_from, currency_to, start_year, end_year):\n",
    "    c = CurrencyRates()\n",
    "    \n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "\n",
    "    rates = {}\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        try:\n",
    "            rate = c.get_rate(currency_from, currency_to, current_date)\n",
    "            rates[current_date.strftime('%Y-%m-%d')] = rate\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch rate for date: {current_date.strftime('%Y-%m-%d')}. Error: {e}\")\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return rates\n",
    "\n",
    "historical_rates = get_historical_rates('EUR', 'RON', 2023, 2023)\n",
    "print(historical_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Exchange Rate'] = df['Data'].apply(lambda x: historical_rates.get(x.strftime('%Y-%m-%d'), None) if pd.notnull(x) else None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[\n",
    "    df['Explicatie'].str.contains('Google', case=False) \n",
    "    & (df['creditor'] != 512.1)\n",
    "    ]\n",
    "filtered_df['Debit_with_exchange'] = filtered_df['Debit'] / filtered_df['Exchange Rate']\n",
    "sum_debit = filtered_df['Debit_with_exchange'].sum()\n",
    "sum_debit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Debitor_First_3'] = df['debitor'].astype(str).str.slice(stop=3)\n",
    "df['Creditor_First_3'] = df['creditor'].astype(str).str.slice(stop=3)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Debitor_First_3'] == '628']\n",
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_descriptions = filtered_df.groupby('Explicatie')['Debit'].sum().reset_index().rename(columns={'Debit': 'Total Debit Amount'})\n",
    "\n",
    "unique_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #3933FF; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Plotting and Presentation\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "> ### We can plot as in excel spreadsheets\n",
    "> ### Here we are plotting the daily cash at bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bank_df = df[df['debitor'].astype(str).str.startswith('512') | df['creditor'].astype(str).str.startswith('512')]\n",
    "\n",
    "bank_df['Amount'] = bank_df.apply(lambda row: row['Debit'] if str(row['debitor']).startswith('512') else -row['Credit'], axis=1)\n",
    "\n",
    "bank_df['Balance'] = bank_df['Amount'].cumsum()\n",
    "\n",
    "plt.plot(bank_df['Balance'])\n",
    "plt.xlabel('Transaction')\n",
    "plt.ylabel('Balance')\n",
    "plt.title('Bank Account Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #3933FF; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Connecting a Generative AI Platform\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "> #### We connect a Generative AI like OpenAI through LanghChain \n",
    "> #### We use a predefined Agent that execute python commands from natural language \n",
    "> #### This will allow us to explore data easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.llms import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set it in your environment variables.\")\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0.5, model=\"gpt-4-1106-preview\"),\n",
    "    df,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"what is this dataframe and for what can be used?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"can you run a Benfords test?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journalEntry = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>ðŸ’¡ Example:</b> Now with <strong>more</strong> data sets, a journal entry and a trial balance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/andi/Desktop/CPD/Balanta de verificare 29112023 1653.xls'\n",
    "\n",
    "df_trialBalance = pd.read_excel(file_path)\n",
    "\n",
    "df_trialBalance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trialBalance = pd.read_excel(file_path, header=9)\n",
    "\n",
    "df_trialBalance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trialBalance.drop(columns=['Unnamed: 2', 'Unnamed: 6'], inplace=True)\n",
    "df_trialBalance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trialBalance.rename(columns=\n",
    "                       {'Unnamed: 0': 'account', \n",
    "                        'Unnamed: 1': 'Description', \n",
    "                        'Debitoare': 'Opening Debit', \n",
    "                        'Creditoare': 'Opening Credit', \n",
    "                        'Debitoare.1': 'Transaction Debit', \n",
    "                        'Creditoare.1': 'Transaction Credit', \n",
    "                        'Debitoare.2': 'Total Transactions Debit', \n",
    "                        'Creditoare.2': 'Total Transactions Credit', \n",
    "                        'Debitoare.3': 'Closing Debit', \n",
    "                        'Creditoare.3': 'Closing Credit'\n",
    "                        }, \n",
    "                       inplace=True)\n",
    "df_trialBalance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trialBalance.dropna(subset=['account'], inplace=True)\n",
    "df_trialBalance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0.5, model=\"gpt-4-1106-preview\"),\n",
    "    [df_trialBalance, df_journalEntry],\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"what informations are in data frames? And how they ralate to eachother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"can you reconcile the information between the two data frames to see if are in agreement?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #3933FF; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Our Knwoledge Base First\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "> #### We bring to our generative model more knowledge\n",
    "> #### Wetransform that knowledge in to vectors so the model ca access it\n",
    "> #### We use this first knwoledge before going forward to the WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Solution(BaseModel):\n",
    "    error: str\n",
    "    solution: str\n",
    "    type: str \n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "\n",
    "    def process(self):\n",
    "        loader = PyPDFLoader(self.pdf_path)\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "        return splitter.split_documents(docs)\n",
    "\n",
    "class VectorStoreCreator:\n",
    "    def __init__(self, persist_directory):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embedding = OpenAIEmbeddings()\n",
    "\n",
    "    def create(self, document_splits):\n",
    "        return Chroma.from_documents(documents=document_splits, embedding=self.embedding, persist_directory=self.persist_directory)\n",
    "\n",
    "class QAChainSetup:\n",
    "    @staticmethod\n",
    "    def setup(vector_store):\n",
    "        chat_model = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0.5)\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are analyzing a technical document to find solutions for specific errors.\n",
    "                        {context}\n",
    "                        Error Query: {question}\n",
    "                        Possible Solution:\"\"\"\n",
    "        )\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            chat_model,\n",
    "            retriever=vector_store.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt_template}\n",
    "        )\n",
    "\n",
    "class ErrorAnalyzer:\n",
    "    def __init__(self, qa_chain, chat_model):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.chat_model = chat_model\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_data(data):\n",
    "        return data.strip() if isinstance(data, str) else data\n",
    "\n",
    "    def analyze_errors(self, error_data):\n",
    "        return [self.process_error(error) for error in error_data]\n",
    "\n",
    "    def process_error(self, error):\n",
    "        response = self.qa_chain({\"query\": error})\n",
    "        text_content = response.result if hasattr(response, 'result') else str(response)\n",
    "        clean_response = self.clean_data(text_content)\n",
    "        \n",
    "        if \"no relevant information found\" in clean_response or clean_response == \"\":\n",
    "            return self.handle_extended_solution(error), \"extended\"\n",
    "        else:\n",
    "            return (error, clean_response), \"in_training\"\n",
    "\n",
    "    def handle_extended_solution(self, error):\n",
    "        gpt_response = self.chat_model({\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": error}]})\n",
    "        clean_gpt_response = self.clean_data(gpt_response['choices'][0]['message']['content'])\n",
    "        return (error, clean_gpt_response)\n",
    "\n",
    "def main():\n",
    "    # Ask the user to input the error message\n",
    "    error_message = input(\"Please enter the error message: \")\n",
    "\n",
    "    document_processor = DocumentProcessor(pdf_path=\"./Manualul-ISA-Vol-I-RO.pdf\")\n",
    "    document_splits = document_processor.process()\n",
    "\n",
    "    vector_store_creator = VectorStoreCreator(persist_directory='docs/chroma/')\n",
    "    vector_store = vector_store_creator.create(document_splits)\n",
    "\n",
    "    qa_chain = QAChainSetup.setup(vector_store)\n",
    "    chat_model = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0.5)\n",
    "\n",
    "    error_analyzer = ErrorAnalyzer(qa_chain, chat_model)\n",
    "\n",
    "    # Process the user-inputted error message\n",
    "    solutions = error_analyzer.analyze_errors([error_message])\n",
    "\n",
    "    # Print the solutions\n",
    "    for solution in solutions:\n",
    "        error, response = solution[0]\n",
    "        print(f\"Error: {error}\\nSolution: {response}\\nType: {solution[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
